---
---

@string{aps = {American Physical Society,}}

@inproceedings{CuiW023,
  author       = {Jiali Cui and
                  Ying Nian Wu and
                  Tian Han},
  abbr = {CVPR},
  selected = {true},
  title        = {Learning Joint Latent Space {EBM} Prior Model for Multi-layer Generator},
  abstract ={This paper studies the fundamental problem of learning multi-layer generator models. The multi-layer generator model builds multiple layers of latent variables as a prior model on top of the generator, which benefits learning complex data distribution and hierarchical representations. However, such a prior model usually focuses on modeling inter-layer relations between latent variables by assuming non-informative (conditional) Gaussian distributions, which can be limited in model expressivity. To tackle this issue and learn more expressive prior models, we propose an energy-based model (EBM) on the joint latent space over all layers of latent variables with the multi-layer generator as its backbone. Such joint latent space EBM prior model captures the intra-layer contextual relations at each layer through layer-wise energy terms, and latent variables across different layers are jointly corrected. We develop a joint training scheme via maximum likelihood estimation (MLE), which involves Markov Chain Monte Carlo (MCMC) sampling for both prior and posterior distributions of the latent variables from different layers. To ensure efficient inference and learning, we further propose a variational training scheme where an inference model is used to amortize the costly posterior MCMC sampling. Our experiments demonstrate that the learned model can be expressive in generating high-quality images and capturing hierarchical features for better outlier detection.},
  pdf = {https://arxiv.org/pdf/2306.06323.pdf}, 
  website = {https://jcui1224.github.io/hierarchical-joint-ebm-proj/},
  bibtex_show = {true},
  booktitle    = {{IEEE/CVF} Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages        = {3603--3612},
  publisher    = {{IEEE}},
  year         = {2023},
  url          = {https://doi.org/10.1109/CVPR52729.2023.00351},
  doi          = {10.1109/CVPR52729.2023.00351},
  timestamp    = {Mon, 28 Aug 2023 16:14:40 +0200},
  biburl       = {https://dblp.org/rec/conf/cvpr/CuiW023.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{KongP0W23,
  author       = {Deqian Kong and
                  Bo Pang and
                  Tian Han and
                  Ying Nian Wu},
  editor       = {Robin J. Evans and
                  Ilya Shpitser},
  abbr = {UAI},
  title        = {Molecule Design by Latent Space Energy-Based Modeling and Gradual
                  Distribution Shifting},
  abstract = {Generation of molecules with desired chemical and biological properties such as high drug-likeness, high binding affinity to target proteins, is critical for drug discovery. In this paper, we propose a probabilistic generative model to capture the joint distribution of molecules and their properties. Our model assumes an energy-based model (EBM) in the latent space. Conditional on the latent vector, the molecule and its properties are modeled by a molecule generation model and a property regression model respectively. To search for molecules with desired properties, we propose a sampling with gradual distribution shifting (SGDS) algorithm, so that after learning the model initially on the training data of existing molecules and their properties, the proposed algorithm gradually shifts the model distribution towards the region supported by molecules with desired values of properties. Our experiments show that our method achieves very strong performances on various molecule design tasks.},
  pdf = {https://proceedings.mlr.press/v216/kong23a/kong23a.pdf}, 
  bibtex_show = {true},
  code = {https://github.com/deqiankong/SGDS}, 
  booktitle    = {Uncertainty in Artificial Intelligence (UAI)},
  series       = {Proceedings of Machine Learning Research},
  volume       = {216},
  pages        = {1109--1120},
  publisher    = {{PMLR}},
  year         = {2023},
  url          = {https://proceedings.mlr.press/v216/kong23a.html},
  timestamp    = {Mon, 28 Aug 2023 17:23:08 +0200},
  biburl       = {https://dblp.org/rec/conf/uai/KongP0W23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{XiaoH22,
  author       = {Zhisheng Xiao and
                  Tian Han},
  title        = {Adaptive Multi-stage Density Ratio Estimation for Learning Latent
                  Space Energy-based Model},
  abstract ={This paper studies the fundamental problem of learning energy-based model (EBM) in the latent space of the generator model. Learning such prior model typically requires running costly Markov Chain Monte Carlo (MCMC). Instead, we propose to use noise contrastive estimation (NCE) to discriminatively learn the EBM through density ratio estimation between the latent prior density and latent posterior density. However, the NCE typically fails to accurately estimate such density ratio given large gap between two densities. To effectively tackle this issue and learn more expressive prior models, we develop the adaptive multi-stage density ratio estimation which breaks the estimation into multiple stages and learn different stages of density ratio sequentially and adaptively. The latent prior model can be gradually learned using ratio estimated in previous stage so that the final latent space EBM prior can be naturally formed by product of ratios in different stages. The proposed method enables informative and much sharper prior than existing baselines, and can be trained efficiently. Our experiments demonstrate strong performances in image generation and reconstruction as well as anomaly detection.},
  pdf = {https://arxiv.org/pdf/2209.08739.pdf},
  bibtex_show = {true},
  abbr = {NeurIPS},
  poster = {poster_neurips22.pdf},
  selected = {true},
  booktitle    = {Advances in Neural Information Processing Systems (NeurIPS) },
  year         = {2022},
  url          = {http://papers.nips.cc/paper\_files/paper/2022/hash/874a4d89f2d04b4bcf9a2c19545cf040-Abstract-Conference.html},
  timestamp    = {Thu, 11 May 2023 17:08:21 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/XiaoH22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{ZhaoQLS0Z22,
  author       = {Yizhou Zhao and
                  Liang Qiu and
                  Pan Lu and
                  Feng Shi and
                  Tian Han and
                  Song{-}Chun Zhu},
  title        = {Learning from the Tangram to Solve Mini Visual Tasks},
  abbr         = {AAAI},
  selected     ={false},
  abstract={Current pre-training methods in computer vision focus on natural images in the daily-life context. However, abstract diagrams such as icons and symbols are common and important in the real world. This work is inspired by Tangram, a game that requires replicating an abstract pattern from seven dissected shapes. By recording human experience in solving tangram puzzles, we present the Tangram dataset and show that a pre-trained neural model on the Tangram helps solve some mini visual tasks based on low-resolution vision. Extensive experiments demonstrate that our proposed method generates intelligent solutions for aesthetic tasks such as folding clothes and evaluating room layouts. The pre-trained feature extractor can facilitate the convergence of few-shot learning tasks on human handwriting and improve the accuracy in identifying icons by their contours. },
  bibtex_show = {true},
  pdf = {https://arxiv.org/pdf/2112.06113.pdf}, 
  code = {https://github.com/yizhouzhao/Tangram}, 
  bibtex_show = {true},
  booktitle    = {Thirty-Sixth {AAAI} Conference on Artificial Intelligence (AAAI) },
  pages        = {3490--3498},
  publisher    = {{AAAI} Press},
  year         = {2022},
  url          = {https://doi.org/10.1609/aaai.v36i3.20260},
  doi          = {10.1609/aaai.v36i3.20260},
  timestamp    = {Mon, 04 Sep 2023 16:50:28 +0200},
  biburl       = {https://dblp.org/rec/conf/aaai/ZhaoQLS0Z22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{LuH022,
  author       = {Chang Lu and
                  Tian Han and
                  Yue Ning},
  title        = {Context-Aware Health Event Prediction via Transition Functions on
                  Dynamic Disease Graphs},
  abbr = {AAAI},
  abstract = {With the wide application of electronic health records (EHR) in healthcare facilities, health event prediction with deep learning has gained more and more attention. A common feature of EHR data used for deep-learning-based predictions is historical diagnoses. Existing work mainly regards a diagnosis as an independent disease and does not consider clinical relations among diseases in a visit. Many machine learning approaches assume disease representations are static in different visits of a patient. However, in real practice, multiple diseases that are frequently diagnosed at the same time reflect hidden patterns that are conducive to prognosis. Moreover, the development of a disease is not static since some diseases can emerge or disappear and show various symptoms in different visits of a patient. To effectively utilize this combinational disease information and explore the dynamics of diseases, we propose a novel context-aware learning framework using transition functions on dynamic disease graphs. Specifically, we construct a global disease co-occurrence graph with multiple node properties for disease combinations. We design dynamic subgraphs for each patient's visit to leverage global and local contexts. We further define three diagnosis roles in each visit based on the variation of node properties to model disease transition processes. Experimental results on two real-world EHR datasets show that the proposed model outperforms state of the art in predicting health events.},
  pdf = {https://arxiv.org/pdf/2112.05195.pdf}, 
  code = {https://github.com/luchang-cs/chet},
  bibtex_show = {true},
  booktitle    = {Thirty-Sixth {AAAI} Conference on Artificial Intelligence (AAAI) },
  pages        = {4567--4574},
  publisher    = {{AAAI} Press},
  year         = {2022},
  url          = {https://doi.org/10.1609/aaai.v36i4.20380},
  doi          = {10.1609/aaai.v36i4.20380},
  timestamp    = {Mon, 04 Sep 2023 16:50:26 +0200},
  biburl       = {https://dblp.org/rec/conf/aaai/LuH022.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{PangNHW21,
  author       = {Bo Pang and
                  Erik Nijkamp and
                  Tian Han and
                  Ying Nian Wu},
  editor       = {Paola Merlo and
                  J{\"{o}}rg Tiedemann and
                  Reut Tsarfaty},
  title        = {Generative Text Modeling through Short Run Inference},
  abbr = {EACL},
  abstract = {Latent variable models for text, when trained successfully, accurately model the data distribution and capture global semantic and syntactic features of sentences. The prominent approach to train such models is variational autoencoders (VAE). It is nevertheless challenging to train and often results in a trivial local optimum where the latent variable is ignored and its posterior collapses into the prior, an issue known as posterior collapse. Various techniques have been proposed to mitigate this issue. Most of them focus on improving the inference model to yield latent codes of higher quality. The present work proposes a short run dynamics for inference. It is initialized from the prior distribution of the latent variable and then runs a small number (e.g., 20) of Langevin dynamics steps guided by its posterior distribution. The major advantage of our method is that it does not require a separate inference model or assume simple geometry of the posterior distribution, thus rendering an automatic, natural and flexible inference engine. We show that the models trained with short run dynamics more accurately model the data, compared to strong language model and VAE baselines, and exhibit no sign of posterior collapse. Analyses of the latent space show that interpolation in the latent space is able to generate coherent sentences with smooth transition and demonstrate improved classification over strong baselines with latent features from unsupervised pretraining. These results together expose a well-structured latent space of our generative model.},
  pdf = {https://arxiv.org/pdf/2106.02513.pdf}, 
  bibtex_show = {true},
  booktitle    = {The 16th Conference of the European Chapter of the
                  Association for Computational Linguistics (EACL) },
  pages        = {1156--1165},
  publisher    = {Association for Computational Linguistics},
  year         = {2021},
  url          = {https://doi.org/10.18653/v1/2021.eacl-main.98},
  doi          = {10.18653/v1/2021.eacl-main.98},
  timestamp    = {Thu, 20 Jan 2022 10:02:57 +0100},
  biburl       = {https://dblp.org/rec/conf/eacl/PangNHW21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Pang0NZW20,
  author       = {Bo Pang and
                  Tian Han and
                  Erik Nijkamp and
                  Song{-}Chun Zhu and
                  Ying Nian Wu},
  editor       = {Hugo Larochelle and
                  Marc'Aurelio Ranzato and
                  Raia Hadsell and
                  Maria{-}Florina Balcan and
                  Hsuan{-}Tien Lin},
  title        = {Learning Latent Space Energy-Based Prior Model},
  abstract = {We propose to learn energy-based model (EBM) in the latent space of a generator model, so that the EBM serves as a prior model that stands on the top-down network of the generator model. Both the latent space EBM and the top-down network can be learned jointly by maximum likelihood, which involves short-run MCMC sampling from both the prior and posterior distributions of the latent vector. Due to the low dimensionality of the latent space and the expressiveness of the top-down network, a simple EBM in latent space can capture regularities in the data effectively, and MCMC sampling in latent space is efficient and mixes well. We show that the learned model exhibits strong performances in terms of image and text generation and anomaly detection.},
  pdf = {https://arxiv.org/pdf/2006.08205.pdf}, 
  website = {https://bpucla.github.io/latent-space-ebm-prior-project/}, 
  bibtex_show = {true},
  abbr = {NeurIPS},
  selected = {true}, 
  booktitle    = {Advances in Neural Information Processing Systems (NeurIPS) },
  year         = {2020},
  url          = {https://proceedings.neurips.cc/paper/2020/hash/fa3060edb66e6ff4507886f9912e1ab9-Abstract.html},
  timestamp    = {Tue, 19 Jan 2021 15:57:02 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/Pang0NZW20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{NijkampH0ZW20,
  author       = {Erik Nijkamp and
                  Mitch Hill and
                  Tian Han and
                  Song{-}Chun Zhu and
                  Ying Nian Wu},
  title        = {On the Anatomy of MCMC-Based Maximum Likelihood Learning of Energy-Based
                  Models},
  abstract = {This study investigates the effects of Markov chain Monte Carlo (MCMC) sampling in unsupervised Maximum Likelihood (ML) learning. Our attention is restricted to the family of unnormalized probability densities for which the negative log density (or energy function) is a ConvNet. We find that many of the techniques used to stabilize training in previous studies are not necessary. ML learning with a ConvNet potential requires only a few hyper-parameters and no regularization. Using this minimal framework, we identify a variety of ML learning outcomes that depend solely on the implementation of MCMC sampling.
On one hand, we show that it is easy to train an energy-based model which can sample realistic images with short-run Langevin. ML can be effective and stable even when MCMC samples have much higher energy than true steady-state samples throughout training. Based on this insight, we introduce an ML method with purely noise-initialized MCMC, high-quality short-run synthesis, and the same budget as ML with informative MCMC initialization such as CD or PCD. Unlike previous models, our energy model can obtain realistic high-diversity samples from a noise signal after training.
On the other hand, ConvNet potentials learned with non-convergent MCMC do not have a valid steady-state and cannot be considered approximate unnormalized densities of the training data because long-run MCMC samples differ greatly from observed images. We show that it is much harder to train a ConvNet potential to learn a steady-state over realistic images. To our knowledge, long-run MCMC samples of all previous models lose the realism of short-run samples. With correct tuning of Langevin noise, we train the first ConvNet potentials for which long-run and steady-state MCMC samples are realistic images.},
  pdf = {https://arxiv.org/pdf/1903.12370.pdf},
  code = {https://github.com/point0bar1/ebm-anatomy},
  bibtex_show ={true},
  abbr = {AAAI},
  booktitle    = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence (AAAI) },
  pages        = {5272--5280},
  publisher    = {{AAAI} Press},
  year         = {2020},
  url          = {https://doi.org/10.1609/aaai.v34i04.5973},
  doi          = {10.1609/aaai.v34i04.5973},
  timestamp    = {Mon, 04 Sep 2023 16:50:26 +0200},
  biburl       = {https://dblp.org/rec/conf/aaai/NijkampH0ZW20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{0001NZPZW20,
  author       = {Tian Han and
                  Erik Nijkamp and
                  Linqi Zhou and
                  Bo Pang and
                  Song{-}Chun Zhu and
                  Ying Nian Wu},
  title        = {Joint Training of Variational Auto-Encoder and Latent Energy-Based
                  Model},
  abstract = {This paper proposes a joint training method to learn both the variational auto-encoder (VAE) and the latent energy-based model (EBM). The joint training of VAE and latent EBM are based on an objective function that consists of three Kullback-Leibler divergences between three joint distributions on the latent vector and the image, and the objective function is of an elegant symmetric and anti-symmetric form of divergence triangle that seamlessly integrates variational and adversarial learning. In this joint training scheme, the latent EBM serves as a critic of the generator model, while the generator model and the inference model in VAE serve as the approximate synthesis sampler and inference sampler of the latent EBM. Our experiments show that the joint training greatly improves the synthesis quality of the VAE. It also enables learning of an energy function that is capable of detecting out of sample examples for anomaly detection.},
  pdf = {https://arxiv.org/pdf/2006.06059.pdf},
  code = {https://github.com/alexzhou907/vae_ebm},
  bibtex_show = {true},
  abbr = {CVPR},
  selected = {true},
  booktitle    = {{IEEE/CVF} Conference on Computer Vision and Pattern Recognition (CVPR) },
  pages        = {7975--7984},
  publisher    = {Computer Vision Foundation / {IEEE}},
  year         = {2020},
  url          = {https://openaccess.thecvf.com/content\_CVPR\_2020/html/Han\_Joint\_Training\_of\_Variational\_Auto-Encoder\_and\_Latent\_Energy-Based\_Model\_CVPR\_2020\_paper.html},
  doi          = {10.1109/CVPR42600.2020.00800},
  timestamp    = {Tue, 31 Aug 2021 14:00:04 +0200},
  biburl       = {https://dblp.org/rec/conf/cvpr/0001NZPZW20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{NijkampP0ZZW20,
  author       = {Erik Nijkamp and
                  Bo Pang and
                  Tian Han and
                  Linqi Zhou and
                  Song{-}Chun Zhu and
                  Ying Nian Wu},
  editor       = {Andrea Vedaldi and
                  Horst Bischof and
                  Thomas Brox and
                  Jan{-}Michael Frahm},
  title        = {Learning Multi-layer Latent Variable Model via Variational Optimization
                  of Short Run {MCMC} for Approximate Inference},
  abstract = {This paper studies the fundamental problem of learning deep generative models that consist of multiple layers of latent variables organized in top-down architectures. Such models have high expressivity and allow for learning hierarchical representations. Learning such a generative model requires inferring the latent variables for each training example based on the posterior distribution of these latent variables. The inference typically requires Markov chain Monte Caro (MCMC) that can be time consuming. In this paper, we propose to use noise initialized non-persistent short run MCMC, such as finite step Langevin dynamics initialized from the prior distribution of the latent variables, as an approximate inference engine, where the step size of the Langevin dynamics is variationally optimized by minimizing the Kullback-Leibler divergence between the distribution produced by the short run MCMC and the posterior distribution. Our experiments show that the proposed method outperforms variational auto-encoder (VAE) in terms of reconstruction error and synthesis quality. The advantage of the proposed method is that it is simple and automatic without the need to design an inference model.},
  pdf = {https://arxiv.org/pdf/1912.01909.pdf},
  website = {https://eriknijkamp.com/project_short_run_inference/}, 
  bibtex_show = {true},
  abbr = {ECCV},
  booktitle    = {16th European Conference on Computer Vision (ECCV) },
  series       = {Lecture Notes in Computer Science},
  volume       = {12351},
  pages        = {361--378},
  publisher    = {Springer},
  year         = {2020},
  url          = {https://doi.org/10.1007/978-3-030-58539-6\_22},
  doi          = {10.1007/978-3-030-58539-6\_22},
  timestamp    = {Tue, 10 Nov 2020 13:22:14 +0100},
  biburl       = {https://dblp.org/rec/conf/eccv/NijkampP0ZZW20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{HanNFHZW19,
  author       = {Tian Han and
                  Erik Nijkamp and
                  Xiaolin Fang and
                  Mitch Hill and
                  Song{-}Chun Zhu and
                  Ying Nian Wu},
  title        = {Divergence Triangle for Joint Training of Generator Model, Energy-Based
                  Model, and Inferential Model},
  abstract = {This paper proposes the divergence triangle as a framework for joint training of generator model, energy-based model and inference model. The divergence triangle is a compact and symmetric (anti-symmetric) objective function that seamlessly integrates variational learning, adversarial learning, wake-sleep algorithm, and contrastive divergence in a unified probabilistic formulation. This unification makes the processes of sampling, inference, energy evaluation readily available without the need for costly Markov chain Monte Carlo methods. Our experiments demonstrate that the divergence triangle is capable of learning (1) an energy-based model with well-formed energy landscape, (2) direct sampling in the form of a generator network, and (3) feed-forward inference that faithfully reconstructs observed as well as synthesized data. The divergence triangle is a robust training method that can learn from incomplete data.},
  pdf = {https://openreview.net/pdf?id=IeEeQOxtS0}, 
  code = {https://github.com/enijkamp/triangle},
  bibtex_show={true},
  abbr = {CVPR},
  selected = {true},
  booktitle    = {{IEEE} Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages        = {8670--8679},
  publisher    = {Computer Vision Foundation / {IEEE}},
  year         = {2019},
  url          = {http://openaccess.thecvf.com/content\_CVPR\_2019/html/Han\_Divergence\_Triangle\_for\_Joint\_Training\_of\_Generator\_Model\_Energy-Based\_Model\_CVPR\_2019\_paper.html},
  doi          = {10.1109/CVPR.2019.00887},
  timestamp    = {Tue, 15 Aug 2023 14:52:14 +0200},
  biburl       = {https://dblp.org/rec/conf/cvpr/HanNFHZW19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{XingHGZW19,
  author       = {Xianglei Xing and
                  Tian Han and
                  Ruiqi Gao and
                  Song{-}Chun Zhu and
                  Ying Nian Wu},
  title        = {Unsupervised Disentangling of Appearance and Geometry by Deformable
                  Generator Network},
  abstract = {We present a deformable generator model to disentangle the appearance and geometric information in purely unsupervised manner. The appearance generator models the appearance related information, including color, illumination, identity or category, of an image, while the geometric generator performs geometric related warping, such as rotation and stretching, through generating displacement of the coordinate of each pixel to obtain the final image. Two generators act upon independent latent factors to extract disentangled appearance and geometric information from images. The proposed scheme is general and can be easily integrated into different generative models. An extensive set of qualitative and quantitative experiments shows that the appearance and geometric information can be well disentangled, and the learned geometric generator can be conveniently transferred to other image datasets to facilitate knowledge transfer tasks.},
  pdf = {https://openaccess.thecvf.com/content_CVPR_2019/papers/Xing_Unsupervised_Disentangling_of_Appearance_and_Geometry_by_Deformable_Generator_Network_CVPR_2019_paper.pdf},
  code ={https://github.com/andyxingxl/Deformable-generator},
  bibtex_show = {true},
  booktitle    = {{IEEE} Conference on Computer Vision and Pattern Recognition (CVPR)},
  abbr = {CVPR},
  pages        = {10354--10363},
  publisher    = {Computer Vision Foundation / {IEEE}},
  year         = {2019},
  url          = {http://openaccess.thecvf.com/content\_CVPR\_2019/html/Xing\_Unsupervised\_Disentangling\_of\_Appearance\_and\_Geometry\_by\_Deformable\_Generator\_Network\_CVPR\_2019\_paper.html},
  doi          = {10.1109/CVPR.2019.01060},
  timestamp    = {Mon, 30 Aug 2021 17:01:14 +0200},
  biburl       = {https://dblp.org/rec/conf/cvpr/XingHGZW19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{HanWW18,
  author       = {Tian Han and
                  Jiawen Wu and
                  Ying Nian Wu},
  editor       = {J{\'{e}}r{\^{o}}me Lang},
  title        = {Replicating Active Appearance Model by Generator Network},
  abstract = {A recent Cell paper [Chang and Tsao, 2017] reports an interesting discovery. For the face stimuli generated by a pre-trained active appearance model (AAM), the responses of neurons in the areas of the primate brain that are responsible for face recognition exhibit strong linear relationship with the shape variables and appearance variables of the AAM that generates the face stimuli. In this paper, we show that this behavior can be replicated by a deep generative model called the generator network, which assumes that the observed signals are generated by latent random variables via a top-down convolutional neural network. Specifically, we learn the generator network from the face images generated by a pre-trained AAM model using variational auto-encoder, and we show that the inferred latent variables of the learned generator network have strong linear relationship with the shape and appearance variables of the AAM model that generates the face images. Unlike the AAM model that has an explicit shape model where the shape variables generate the control points or landmarks, the generator network has no such shape model and shape variables. Yet the generator network can learn the shape knowledge in the sense that some of the latent variables of the learned generator network capture the shape variations in the face images generated by AAM.},
  pdf = {https://arxiv.org/pdf/1805.08704.pdf},
  bibtex_show ={true},
  abbr = {IJCAI},
  booktitle    = {the Twenty-Seventh International Joint Conference on
                  Artificial Intelligence (IJCAI) },
  pages        = {2205--2211},
  publisher    = {ijcai.org},
  year         = {2018},
  url          = {https://doi.org/10.24963/ijcai.2018/305},
  doi          = {10.24963/ijcai.2018/305},
  timestamp    = {Mon, 18 Nov 2019 19:15:55 +0100},
  biburl       = {https://dblp.org/rec/conf/ijcai/HanWW18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{HanLZW17,
  author       = {Tian Han and
                  Yang Lu and
                  Song{-}Chun Zhu and
                  Ying Nian Wu},
  editor       = {Satinder Singh and
                  Shaul Markovitch},
  title        = {Alternating Back-Propagation for Generator Network},
  abstract = {This paper proposes an alternating back-propagation algorithm for learning the generator network model. The model is a non-linear generalization of factor analysis. In this model, the mapping from the continuous latent factors to the observed signal is parametrized by a convolutional neural network. The alternating back-propagation algorithm iterates the following two steps: (1) Inferential back-propagation, which infers the latent factors by Langevin dynamics or gradient descent. (2) Learning back-propagation, which updates the parameters given the inferred latent factors by gradient descent. The gradient computations in both steps are powered by back-propagation, and they share most of their code in common. We show that the alternating back-propagation algorithm can learn realistic generator models of natural images, video sequences, and sounds. Moreover, it can also be used to learn from incomplete or indirect training data.},
  pdf = {https://arxiv.org/pdf/1606.08571.pdf},
  website = {http://www.stat.ucla.edu/~ywu/ABP/main.html},
  bibtex_show = {true},
  abbr = {AAAI},
  selected = {true},
  booktitle    = {the Thirty-First {AAAI} Conference on Artificial Intelligence (AAAI)},
  pages        = {1976--1984},
  publisher    = {{AAAI} Press},
  year         = {2017},
  url          = {https://doi.org/10.1609/aaai.v31i1.10902},
  doi          = {10.1609/aaai.v31i1.10902},
  timestamp    = {Mon, 04 Sep 2023 16:50:24 +0200},
  biburl       = {https://dblp.org/rec/conf/aaai/HanLZW17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
